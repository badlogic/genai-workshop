# Resources

Here you find supplemental information to get a deeper understanding of topics covered during the workshop.

## Python & Jupyter

- [Google's Python Class](https://developers.google.com/edu/python), an excellent, free Python course for people with a little programming background
- [Google Colaboratory Beginner's Guide](https://reybahl.medium.com/beginners-guide-to-google-colaboratory-e13805a2a1c6), teaches the essentials of Google Colaboratory, which itself is based on Jupyter, with some slight modifications/improvements/changes.
- [Jupyter Documentation](https://docs.jupyter.org/en/latest/index.html),

## General Machine Learning & Transformers

- [Understanding Deep Learning](https://udlbook.github.io/udlbook/), free foundational book
- [Variational Autoencoders](https://youtu.be/9zKuYvjFFS8?si=7golMS_YgdwWRe4y)
- [Illustrated BERT](https://jalammar.github.io/illustrated-bert/)
- [BertViz](https://github.com/jessevig/bertviz), visualize attention in NLP models
- [Illustrated Guide to Transformer Neural Networks](https://www.youtube.com/watch?v=4Bdc55j80l8)
- [BERT explained - A list of Frequently Asked Questions](https://yashuseth.wordpress.com/2019/06/12/bert-explained-faqs-understand-bert-working/)
- [Transformers and Large Language Models](https://web.stanford.edu/~jurafsky/slp3/10.pdf)
- [Benchmarking Large Language Models for Log Analysis, Security, and Interpretation](https://arxiv.org/pdf/2311.14519v1.pdf)
- [Stanford CS25: Transformers United V3](https://web.stanford.edu/class/cs25/), see also the [YouTube playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)

## Prompt Engineering

- [OpenAI prompt engineering guide](https://platform.openai.com/docs/guides/prompt-engineering)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

## Retrieval-augmented generation

- [Building RAG-based LLM Applications for Production](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)
- [Building RAG Agents with LLMs](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/building-rag-agents-with-llms-dli-course), free online course by NVIDIA
- [RETRO - Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)
- [The illustrated retrieval transformer](https://jalammar.github.io/illustrated-retrieval-transformer/)
- [Better RAG](https://huggingface.co/blog/hrishioa/retrieval-augmented-generation-1-basics)
- [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)

## Custom Coding Assistants ala GitHub Copilot

- [aider](https://aider.chat/), an AI pair programming tool for your own code base.
- [localpilot](https://github.com/danielgross/localpilot/tree/main), a proof of concept showing how to (ab-)use the GitHub Copilot extension for Visual Studio Code by pointing it to a locally run LLM.
- [Continue](https://github.com/continuedev/continue)

## Hosting & Deployment of models

- [LLMPerf](https://github.com/ray-project/llmperf-leaderboard?tab=readme-ov-file), benchmark of various LLM inference providers.

## Fine-Tuning

- [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs (January 2024)](https://arxiv.org/abs/2312.05934)
- [Fine-tuning is for form, not facts](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts)
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl), a popular fine-tuning framework.
- [Fine-tuning guide by Modal](https://github.com/modal-labs/llm-finetuning) demonstrating best practices when using Axolotl
- [Hugging Face Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), an exceptionally simple to use framework to traing and fine-tune machine learning models.
- [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/en/index), framework to distribute training and inference
- [Hugging Face Transformer Reinforcement Learning](https://github.com/huggingface/trl), a full stack library where we provide a set of tools to train transformer language models and stable diffusion models with Reinforcement Learning
- [DeepSpeed](https://github.com/microsoft/DeepSpeed), deep learning optimization software suite, with integrations for popular deep learning frameworks like [Hugging Face Transformers](https://huggingface.co/docs/transformers/main/main_classes/deepspeed)
- [Fine-tuning CodeLlama using Quanized Low-Rank Adaptation on Amazon SageMaker](https://medium.com/@philippkai/natural-language-to-sql-fine-tuning-codellama-with-amazon-sagemaker-part-1-3e1eb0fd1b11)
- [Fine-tuning a Code LLM on Custom Code on a single GPU](https://huggingface.co/learn/cookbook/fine_tuning_code_llm_on_single_gpu), demonstrates the [FIM transformations](https://arxiv.org/pdf/2207.14255.pdf) to turn a causal LLM to an infilling LLM useful for code completions.
- [Personal Copilot: Train your own coding assistant](https://huggingface.co/blog/personal-copilot)
- [Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2)
- [Hugging Face Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)
- [Fine-tune LLMs for natural language to SQL completions](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl)
- [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406)

## Other reference compilations

- [Large Language Model Course](https://github.com/mlabonne/llm-course), collection of LLM related resources
